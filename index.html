<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0055)https://qmul-survface.github.io/QMUL-SurvFace/index.htm -->
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>QMUL-SurvFace</title>
      <meta name="description" content="To facilitate more studies on developing face recognition models that are effective and robust 
         for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, 
         which we call the QMUL-SurvFace benchmark. 
         This new benchmark is the largest and more importantly the only true surveillance face recognition benchmark 
         to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling 
         of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities 
         captured in real-world uncooperative surveillance scenes over wide space and time. 
         Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, 
         owing to a large number of non-target people (distractors) appearing open spaced scenes.">
      <meta name="keywords" content="QMUL SurvFace; 
         QMUL Surveillance Face Recognition Challenge; QMUL Surveillance Face Challenge;
         QMUL Surveillance Face Recognition Benchmark; QMUL Surveillance Face Benchmark;
         face recognition; surveillance face recognition; face identification; face verification; 
         open-set recognition; closed-set recognition; 
         native low-resolution; genuine low-resolution;
         true low-resolution; false low-resolution;
         artificial down-sampling; artificial sub-sampling; artificial low-resolution;
         very low-resolution; extreme low-resolution;
         low quality; low-quality; poor quality; poor-quality;
         super-resolution; high scale super-resolution;
         surveillance face; native face; poor quality faces; 
         large scale face search; large scale face matching; cross-camera matching;
         benchmark; computer vision;">
      <!-- Fonts and stuff -->
      <link href="./QMUL-SurvFace_files/project.css" rel="stylesheet">
      <link href="./QMUL-SurvFace_files/iconize.css" rel="stylesheet">
      <link href="./images/new_icon.png" rel="shortcut icon">
      <script type="text/javascript" async="" src="./QMUL-SurvFace_files/ga.js"></script>
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144">
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114">
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72">
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-57-precomposed.png" rel="apple-touch-icon-precomposed">
   </head>
   <body>
      <div id="content">
         <div id="content-inner">
            <div class="section head">
               <p>
               <h1 id="surveillance-face-recognition-challenge">QMUL-SurvFace: Surveillance Face Recognition Challenge</h1>
               </p>
               <div class="affiliations">
                  <p><a href="http://www.eecs.qmul.ac.uk/~zc302/">Zhiyi Cheng</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~xiatian/">Xiatian Zhu</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
                  <p><a href="http://vision.eecs.qmul.ac.uk/">Computer Vision Group,  </a >
                     <a href="http://www.eecs.qmul.ac.uk/">School of Electronic Engineering and Computer Science,  </a >
                     <a href="http://www.qmul.ac.uk/">Queen Mary University of London</a >
               </div>
               <ul id="tabs">
                  <li><a href="https://qmul-survface.github.io/index.html" name="#tab1" id="current">Home</a></li>
                  <li><a href="https://qmul-survface.github.io/protocols.html" name="#tab2">Protocols</a></li>
                  <li><a href="https://qmul-survface.github.io/benchmark.html" name="#tab3">Leaderboard</a></li>
               </ul>
            </div>
            <center><img style="width: 95%;" src="./images/survface_pairs-min.png" alt="QMUL SurvFace"></center>
            <h2 id="description">Description</h2>
            <p>To facilitate more studies for developing face recognition methods that are effective and robust against low-resolution surveillance facial images, 
               a new <em>Surveillance Face Recognition</em> challenge, <strong>QMUL-SurvFace</strong>, is introduced. 
               This new challenge is the largest and more importantly the only true surveillance face recognition benchmark to our best knowledge, 
               where low-resolution face images are native and not synthesised by artificial down-sampling of native high-resolution images. 
               This challenge contains <strong>463,507</strong> face images of <strong>15,573</strong> distinct identities captured in real-world 
               uncooperative surveillance scenes across wide space and time. 
               Face recognition is generally more difficult in an
               open-set setting which is typical for surveillance person search scenarios,
               owing to an arbitrarily large number of non-target people
               (distractors) appearing over open space and unconstrained time.
            </p>
            <h2 id="news">News</h2>
            <p>
            <ul>
	       <li><strong>August 29, 2018:</strong> Updated the evaluation results of the state-of-the-art face recognition methods. </li>
               <li><strong>June 01, 2018:</strong> QMUL-SurvFace dataset, the evaluation protocol, and test codes are released. </li>
            </ul>
            </p>
            <h2 id="download">Download</h2>
            <p>QMUL-SurvFace Dataset and Evaluation Codes (389MB): 
               [<a href="https://drive.google.com/open?id=13ch6BPaexlKt8gXB_I8aX7p1G3yPm2Bl" target="_blank">Google Drive</a>] 
               [<a href="https://pan.baidu.com/s/1O55042SMxLYqBWdGGQ_4_g" target="_blank">Baidu Cloud</a>]
            </p>
            <h2 id="citation">Citation</h2>
            <pre>
	    	<em>Surveillance Face Recognition Challenge.</em>
		Zhiyi Cheng, Xiatian Zhu and Shaogang Gong.
		Technical Report, 2018. <a href="https://arxiv.org/abs/1804.09691">Paper</a> <a href="QMUL-SurvFace_bibtex.html">Bibtex</a> 
	    </pre>
            <h2 id="Related Datasets">Related Datasets</h2>
            <p>We list below existing surveillance face recognition datasets. More extensive comparisons of face recognition datasets can be found in the <a href="https://arxiv.org/pdf/1804.09691.pdf">paper.</a></p>
            <ul>
               <li><a href="http://vast.uccs.edu/Opensetface/">UCCS Challenge</a>:  UCCS is a high-resolution surveillance face detection and recognition challenge. 
                  It contains 1,732 identities captured by a Canon 7D camera fitted with Sigma 800mm F5.6 EX APO DG HSM lens.
               </li>
               <li><a href="http://www.scface.org/">SCface Dataset</a>: SCface is one of the earliest surveillance face recognition datasets. 
                  It consists of 4,160 static images (in visible and infrared spectrum) of 130 identities.
               </li>
            </ul>
            <h2 id="Licence">Licence</h2>
            <p>Notice that the QMUL-SurvFace challenge is made available for research purposes.
               All the images were collected from the existing person re-identification datasets, 
               and the copyright belongs to the original owners. 
            </p>
            <h2 id="contact">Contact</h2>
            <p>Please feel free to send any questions, comments, and evaluation results with a brief method description to Zhiyi Cheng at <strong>z.cheng@qmul.ac.uk</strong>.</p>
            <div class="footer">
	        <div class="clearfix">
		  <div class="leftbox">
                      <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=ZMooCrePyovHLZMPtSdcJlZgBCLOXkBOLp3sH4Fn5b0&cl=ffffff&w=a"></script>
		  </div>

		  <div class="rightbox">
		      <a href="https://m.maploco.com/details/fa55v386"><img style="border:0px; width:220px;" src="https://www.maploco.com/vmap/s/9848598.png" alt="Locations of Site Visitors" title="Locations of Site Visitors"/></a>  
		  </div>
		</div>
           </div>
         </div>
         
      </div>
   </body>
</html>
