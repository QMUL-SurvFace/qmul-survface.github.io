<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0058)https://qmul-survface.github.io/QMUL-SurvFace_results.html -->
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>QMUL-SurvFace</title>
      <meta name="description" content="To facilitate more studies on developing face recognition models that are effective and robust 
         for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, 
         which we call the QMUL-SurvFace benchmark. 
         This new benchmark is the largest and more importantly the only true surveillance face recognition benchmark 
         to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling 
         of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities 
         captured in real-world uncooperative surveillance scenes over wide space and time. 
         Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, 
         owing to a large number of non-target people (distractors) appearing open spaced scenes.">
      <meta name="keywords" content="QMUL SurvFace; 
         QMUL Surveillance Face Recognition Challenge; QMUL Surveillance Face Challenge;
         QMUL Surveillance Face Recognition Benchmark; QMUL Surveillance Face Benchmark;
         face recognition; surveillance face recognition; face identification; face verification; 
         open-set recognition; closed-set recognition; 
         native low-resolution; genuine low-resolution;
         true low-resolution; false low-resolution;
         artificial down-sampling; artificial sub-sampling; artificial low-resolution;
         very low-resolution; extreme low-resolution;
         low quality; low-quality; poor quality; poor-quality;
         super-resolution; high scale super-resolution;
         surveillance face; native face; poor quality faces; 
         large scale face search; large scale face matching; cross-camera matching;
         benchmark; computer vision;">
      <!-- Fonts and stuff -->
      <link href="./QMUL-SurvFace_files/css" rel="stylesheet" type="text/css">
      <link rel="stylesheet" type="text/css" href="./QMUL-SurvFace_files/project.css" media="screen">
      <link rel="stylesheet" type="text/css" media="screen" href="./QMUL-SurvFace_files/iconize.css">
      <link rel="shortcut icon" href="./images/new_icon.png">
      <script type="text/javascript" async="" src="./QMUL-SurvFace_files/ga.js"></script><script async="" src="./QMUL-SurvFace_files/prettify.js"></script><!-- End Jekyll SEO tag -->
      <style>
         table td
         {
             text-align: center; /*Aligns all content of td elements to center*/
         }
         
         table th
         {
             text-align: center; /*Aligns all content of th elements to center*/
         }
         
         table td a
         {
             display: inline-block; /*Behaves like a div, but can be placed inline*/
             width: 100%; /*Full width of parent*/
             height: 100%; /*Full height of parent*/
             text-align: center; /*Centers content*/
         }
      </style>
   </head>
   <body>
      <div id="content">
         <div id="content-inner">
            <div class="section head">
               <p>
               </p>
               <h1 id="surveillance-face-recognition-challenge">QMUL-SurvFace: Surveillance Face Recognition Challenge</h1>
               <p></p>
               <div class="affiliations">
                  <p><a href="http://www.eecs.qmul.ac.uk/~zc302/">Zhiyi Cheng</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~xiatian/">Xiatian Zhu</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
                  </p>
                  <p><a href="http://vision.eecs.qmul.ac.uk/">Computer Vision Group,  </a>
                     <a href="http://www.eecs.qmul.ac.uk/">School of Electronic Engineering and Computer Science,  </a>
                     <a href="http://www.qmul.ac.uk/">Queen Mary University of London</a>
                  </p>
               </div>
               <ul id="tabs">
                  <li><a href="https://qmul-survface.github.io/index.html" name="#tab1">Home</a></li>
                  <li><a href="https://qmul-survface.github.io/protocols.html" name="#tab2">Protocols</a></li>
                  <li><a href="https://qmul-survface.github.io/benchmark.html" name="#tab3" id="current">Benchmark</a></li>
               </ul>
            </div>
            <div class="tab-frame">
               <input type="radio" checked name="tab" id="tab1">
               <label for="tab1">
                  <h3><strong>Identification</strong></h3>
               </label>
               <input type="radio" name="tab" id="tab2">
               <label for="tab2">
                  <h3><strong>Verification</strong></h3>
               </label>
               <div class="tab">
                  <h3>Identification</h3>
                  <table align="center" width="100%" style="margin: 0px auto;" class="justify" cellpadding="0" cellspacing="0">
                     <thead>
                        <tr>
                           <th>Algorithm</th>
                           <th>Publication</th>
                           <th>TPIR@FPIR=30%</th>
                           <th>TPIR@FPIR=20%</th>
                           <th>TPIR@FPIR=10%</th>
                           <th>AUC</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <td><a href="https://pdfs.semanticscholar.org/8774/e206564df3bf9050f8c2be6b434cc2469c5b.pdf?_ga=2.147452215.187194200.1528491351-695318045.1528491351"><strong>CentreFace</strong></a></td>
                           <td>ECCV 2016</td>
                           <td>24.5%</td>
                           <td>19.6%</td>
                           <td>13.2%</td>
                           <td>33.7%</td>
                        </tr>
                        <tr>
                           <td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf"><strong>SphereFace</strong></a></td>
                           <td>CVPR 2017</td>
                           <td>15.8%</td>
                           <td>11.8%</td>
                           <td>6.8%</td>
                           <td>21.5%</td>
                        </tr>
                        <tr>
                           <td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"><strong>FaceNet<strong></a></td>
                           <td>CVPR 2015</td>
                           <td>12.0%</td>
                           <td>7.5%</td>
                           <td>3.9%</td>
                           <td>18.6%</td>
                        </tr>
                        <tr>
                           <td><a href="http://papers.nips.cc/paper/5416-deep-learning-face-representation-by-joint-identification-verification.pdf"><strong>DeepID2</strong></a></td>
                           <td>NIPS 2014</td>
                           <td>12.0%</td>
                           <td>7.5%</td>
                           <td>2.8%</td>
                           <td>18.7%</td>
                        </tr>
                        <tr>
                           <td><a href="http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf"><strong>VggFace</strong></a></td>
                           <td>BMVC 2015</td>
                           <td>6.5%</td>
                           <td>4.8%</td>
                           <td>2.5%</td>
                           <td>9.6%</td>
                        </tr>
                     </tbody>
                  </table>
                  <br>  
               </div>
               <div class="tab">
                  <h3>Verification</h3>
                  <table align="center" width="100%" style="margin: 0px auto;" class="justify" cellpadding="0" cellspacing="0">
                     <thead >
                        <tr>
                           <th>Algorithm</th>
                           <th>Publication</th>
                           <th>TAR@FAR=30%</th>
                           <th>TAR@FAR=10%</th>
                           <th>TAR@FAR=1%</th>
                           <th>TAR@FAR=0.1%</th>
                           <th>AUC</th>
                           <th>Ave. Accuracy</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <td><a href="https://pdfs.semanticscholar.org/8774/e206564df3bf9050f8c2be6b434cc2469c5b.pdf?_ga=2.147452215.187194200.1528491351-695318045.1528491351"><strong>CentreFace</strong></a></td>
                           <td>ECCV 2016</td>
                           <td>95.4%</td>
                           <td>85.8%</td>
                           <td>62.4%</td>
                           <td>40.7%</td>
                           <td>94.9%</td>
                           <td>87.9%</td>
                        </tr>
                        <tr>
                           <td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"><strong>FaceNet</strong></a></td>
                           <td>CVPR 2015</td>
                           <td>94.5%</td>
                           <td>80.8%</td>
                           <td>48.2%</td>
                           <td>20.8%</td>
                           <td>93.7%</td>
                           <td>85.7%</td>
                        </tr>
                        <tr>
                           <td><a href="http://papers.nips.cc/paper/5416-deep-learning-face-representation-by-joint-identification-verification.pdf"><strong>DeepID2</strong></a></td>
                           <td>NIPS 2014</td>
                           <td>80.4%</td>
                           <td>63.4%</td>
                           <td>36.6%</td>
                           <td>22.9%</td>
                           <td>84.6%</td>
                           <td>76.9%</td>
                        </tr>
                        <tr>
                           <td> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf"><strong>SphereFace</strong></a></td>
                           <td>CVPR 2017</td>
                           <td>78.9%</td>
                           <td>64.8%</td>
                           <td>39.0%</td>
                           <td>22.5%</td>
                           <td>83.5%</td>
                           <td>77.5%</td>
                        </tr>
                        <tr>
                           <td><a href="http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf"><strong>VggFace</strong></a></td>
                           <td>BMVC 2015</td>
                           <td>64.7%</td>
                           <td>42.4%</td>
                           <td>15.0%</td>
                           <td>4.4%</td>
                           <td>74.2%</td>
                           <td>67.6%</td>
                        </tr>
                     </tbody>
                  </table>
                  <br>
                  <br>
               </div>
            </div>
         </div>
      </div>
   </body>
</html>
