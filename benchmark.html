<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0058)https://qmul-survface.github.io/QMUL-SurvFace_results.html -->
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>QMUL-SurvFace</title>
      <meta name="description" content="To facilitate more studies on developing face recognition models that are effective and robust 
         for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, 
         which we call the QMUL-SurvFace benchmark. 
         This new benchmark is the largest and more importantly the only true surveillance face recognition benchmark 
         to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling 
         of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities 
         captured in real-world uncooperative surveillance scenes over wide space and time. 
         Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, 
         owing to a large number of non-target people (distractors) appearing open spaced scenes.">
      <meta name="keywords" content="QMUL SurvFace; 
         QMUL Surveillance Face Recognition Challenge; QMUL Surveillance Face Challenge;
         QMUL Surveillance Face Recognition Benchmark; QMUL Surveillance Face Benchmark;
         face recognition; surveillance face recognition; face identification; face verification; 
         open-set recognition; closed-set recognition; 
         native low-resolution; genuine low-resolution;
         true low-resolution; false low-resolution;
         artificial down-sampling; artificial sub-sampling; artificial low-resolution;
         very low-resolution; extreme low-resolution;
         low quality; low-quality; poor quality; poor-quality;
         super-resolution; high scale super-resolution;
         surveillance face; native face; poor quality faces; 
         large scale face search; large scale face matching; cross-camera matching;
         benchmark; computer vision;">
      <!-- Fonts and stuff -->
      <link href="./QMUL-SurvFace_files/project.css" rel="stylesheet" type="text/css" media="screen">
      <link href="./QMUL-SurvFace_files/iconize.css" rel="stylesheet" type="text/css" media="screen">
      <link rel="shortcut icon" href="./images/new_icon.png">
      <script type="text/javascript" async="" src="./QMUL-SurvFace_files/ga.js"></script><script async="" src="./QMUL-SurvFace_files/prettify.js"></script><!-- End Jekyll SEO tag -->
      <style>
         table td
         {
             text-align: center; /*Aligns all content of td elements to center*/
         }
         
         table th
         {
             text-align: center; /*Aligns all content of th elements to center*/
         }
         
         table td a
         {
             display: inline-block; /*Behaves like a div, but can be placed inline*/
             width: 100%; /*Full width of parent*/
             height: 100%; /*Full height of parent*/
             text-align: center; /*Centers content*/
         }
      </style>
   </head>
   <body>
      <div id="content">
         <div id="content-inner">
            <div class="section head">
               <p>
               </p>
               <h1 id="surveillance-face-recognition-challenge">QMUL-SurvFace: Surveillance Face Recognition Challenge</h1>
               <p></p>
               <div class="affiliations">
                  <p><a href="http://www.eecs.qmul.ac.uk/~zc302/">Zhiyi Cheng</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~xiatian/">Xiatian Zhu</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
                  </p>
                  <p><a href="http://vision.eecs.qmul.ac.uk/">Computer Vision Group,  </a>
                     <a href="http://www.eecs.qmul.ac.uk/">School of Electronic Engineering and Computer Science,  </a>
                     <a href="http://www.qmul.ac.uk/">Queen Mary University of London</a>
                  </p>
               </div>
               <ul id="tabs">
                  <li><a href="https://qmul-survface.github.io/index.html" name="#tab1">Home</a></li>
                  <li><a href="https://qmul-survface.github.io/protocols.html" name="#tab2">Protocols</a></li>
                  <li><a href="https://qmul-survface.github.io/benchmark.html" name="#tab3" id="current">Leaderboard</a></li>
               </ul>
            </div>
            <div class="tab-frame">
               <input type="radio" checked name="tab" id="tab1">
               <label for="tab1">
                  <h3><strong>Face Identification Results</strong></h3>
               </label>
               <input type="radio" name="tab" id="tab2">
               <label for="tab2">
                  <h3><strong>Face Verification Results</strong></h3>
               </label>
               <div class="tab">
                  <table align="center" width="100%" style="margin: 0px auto;" class="justify" cellpadding="0" cellspacing="0">
                     <thead>
                        <tr>
                           <th>Algorithm</th>
                           <th>Publication</th>
                           <th>TPIR@FPIR=30%</th>
                           <th>TPIR@FPIR=20%</th>
                           <th>TPIR@FPIR=10%</th>
                           <th>TPIR@FPIR=1%</th>
                           <th>AUC</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <td><a href="https://pdfs.semanticscholar.org/8774/e206564df3bf9050f8c2be6b434cc2469c5b.pdf?_ga=2.147452215.187194200.1528491351-695318045.1528491351"><strong>CentreFace</strong></a></td>
                           <td>ECCV 2016</td>
                           <td>27.3%</td>
                           <td>21.0%</td>
                           <td>13.8%</td>
                           <td>3.1%</td>
                           <td>37.3%</td>
                        </tr>
                        <tr>
                           <td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf"><strong>SphereFace</strong></a></td>
                           <td>CVPR 2017</td>
                           <td>21.3%</td>
                           <td>15.7%</td>
                           <td>8.3%</td>
                           <td>1.0%</td>
                           <td>28.1%</td>
                        </tr>
                        <tr>
                           <td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"><strong>FaceNet<strong></a></td>
                           <td>CVPR 2015</td>
                           <td>12.7%</td>
                           <td>8.1%</td>
                           <td>4.3%</td>
                           <td>1.0%</td>
                           <td>19.8%</td>
                        </tr>
                        <tr>
                           <td><a href="http://papers.nips.cc/paper/5416-deep-learning-face-representation-by-joint-identification-verification.pdf"><strong>DeepID2</strong></a></td>
                           <td>NIPS 2014</td>
                           <td>12.8%</td>
                           <td>8.1%</td>
                           <td>3.4%</td>
                           <td>0.8%</td>
                           <td>20.8%</td>
                        </tr>
                        <tr>
                           <td><a href="http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf"><strong>VggFace</strong></a></td>
                           <td>BMVC 2015</td>
                           <td>6.5%</td>
                           <td>4.8%</td>
                           <td>2.5%</td>
                           <td>0.2%</td>
                           <td>9.6%</td>
                        </tr>
                     </tbody>
                  </table>
                  <br> 
                  <p> 1. CentreFace - Wen, Y., Zhang, K., Li, Z., & Qiao, Y. (2016, October). A discriminative feature learning approach for deep face recognition. In European Conference on Computer Vision (pp. 499-515). Springer, Cham. </p>
                  <p> 2. SphereFace - Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., & Song, L. (2017, July). Sphereface: Deep hypersphere embedding for face recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Vol. 1). </p>
                  <p> 3. FaceNet - Schroff, F., Kalenichenko, D., & Philbin, J. (2015). Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 815-823). </p>
                  <p> 4. DeepID2 - Sun, Y., Chen, Y., Wang, X., & Tang, X. (2014). Deep learning face representation by joint identification-verification. In Advances in neural information processing systems (pp. 1988-1996). </p>
                  <p> 5. VggFace - Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015, September). Deep Face Recognition. In BMVC (Vol. 1, No. 3, p. 6). </p>
                  <br>
               </div>
               <div class="tab">
                  <table align="center" width="100%" style="margin: 0px auto;" class="justify" cellpadding="0" cellspacing="0">
                     <thead >
                        <tr>
                           <th>Algorithm</th>
                           <th>Publication</th>
                           <th>TAR@FAR=30%</th>
                           <th>TAR@FAR=10%</th>
                           <th>TAR@FAR=1%</th>
                           <th>TAR@FAR=0.1%</th>
                           <th>AUC</th>
                           <th>Ave. Accuracy</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <td><a href="https://pdfs.semanticscholar.org/8774/e206564df3bf9050f8c2be6b434cc2469c5b.pdf?_ga=2.147452215.187194200.1528491351-695318045.1528491351"><strong>CentreFace</strong></a></td>
                           <td>ECCV 2016</td>
                           <td>95.2%</td>
                           <td>86.0%</td>
                           <td>53.3%</td>
                           <td>26.8%</td>
                           <td>94.8%</td>
                           <td>88.0%</td>
                        </tr>
                        <tr>
                           <td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"><strong>FaceNet</strong></a></td>
                           <td>CVPR 2015</td>
                           <td>94.6%</td>
                           <td>79.9%</td>
                           <td>40.3%</td>
                           <td>12.7%</td>
                           <td>93.5%</td>
                           <td>85.3%</td>
                        </tr>
                        <tr>
                           <td><a href="http://papers.nips.cc/paper/5416-deep-learning-face-representation-by-joint-identification-verification.pdf"><strong>DeepID2</strong></a></td>
                           <td>NIPS 2014</td>
                           <td>80.6%</td>
                           <td>60.0%</td>
                           <td>28.2%</td>
                           <td>13.4%</td>
                           <td>84.1%</td>
                           <td>76.1%</td>
                        </tr>
                        <tr>
                           <td> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf"><strong>SphereFace</strong></a></td>
                           <td>CVPR 2017</td>
                           <td>80.0%</td>
                           <td>63.6%</td>
                           <td>34.1%</td>
                           <td>15.6%</td>
                           <td>83.9%</td>
                           <td>77.6%</td>
                        </tr>
                        <tr>
                           <td><a href="http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf"><strong>VggFace</strong></a></td>
                           <td>BMVC 2015</td>
                           <td>83.2%</td>
                           <td>63.0%</td>
                           <td>20.1%</td>
                           <td>4.0%</td>
                           <td>85.0%</td>
                           <td>78.0%</td>
                        </tr>
                     </tbody>
                  </table>
                  <br>
                  <p> 1. CentreFace - Wen, Y., Zhang, K., Li, Z., & Qiao, Y. (2016, October). A discriminative feature learning approach for deep face recognition. In European Conference on Computer Vision (pp. 499-515). Springer, Cham. </p>
                  <p> 2. FaceNet - Schroff, F., Kalenichenko, D., & Philbin, J. (2015). Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 815-823). </p>
                  <p> 3. DeepID2 - Sun, Y., Chen, Y., Wang, X., & Tang, X. (2014). Deep learning face representation by joint identification-verification. In Advances in neural information processing systems (pp. 1988-1996). </p>
                  <p> 4. SphereFace - Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., & Song, L. (2017, July). Sphereface: Deep hypersphere embedding for face recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Vol. 1). </p>
                  <p> 5. VggFace - Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015, September). Deep Face Recognition. In BMVC (Vol. 1, No. 3, p. 6). </p>
                  <br>
               </div>
            </div>
         </div>
      </div>
   </body>
</html>
