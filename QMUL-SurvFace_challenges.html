<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0055)https://qmul-survface.github.io/QMUL-SurvFace/index.htm -->
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>QMUL-SurvFace</title>
      <meta name="description" content="To facilitate more studies on developing face recognition models that are effective and robust 
         for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, 
         which we call the QMUL-SurvFace benchmark. 
         This new benchmark is the largest and more importantly the only true surveillance face recognition benchmark 
         to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling 
         of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities 
         captured in real-world uncooperative surveillance scenes over wide space and time. 
         Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, 
         owing to a large number of non-target people (distractors) appearing open spaced scenes.">
      <meta name="keywords" content="face recognition; surveillance face; surveillance face recognition; open-set recognition; closed-set recognition; low-resolution; super-resolution; benchmarks; computer vision;">
      <!-- Fonts and stuff -->
      <link href="./QMUL-SurvFace_files/css" rel="stylesheet" type="text/css">
      <link rel="stylesheet" type="text/css" href="./QMUL-SurvFace_files/project.css" media="screen">
      <link rel="stylesheet" type="text/css" media="screen" href="./QMUL-SurvFace_files/iconize.css">
      <link rel="shortcut icon" href="./images/icon_fav.png">
      <script type="text/javascript" async="" src="./QMUL-SurvFace_files/ga.js"></script><script async="" src="./QMUL-SurvFace_files/prettify.js"></script><!-- End Jekyll SEO tag -->
   </head>
   <body>
      <div id="content">
         <div id="content-inner">
            <div class="section head">
               <p>
               <h1 id="surveillance-face-recognition-challenge">Surveillance Face Recognition Challenge</h1>
               </p>
               <div class="affiliations">
                  <p><a href="http://www.eecs.qmul.ac.uk/~zc302/">Zhiyi Cheng</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~xiatian/">Xiatian Zhu</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
                  <p><a href="http://vision.eecs.qmul.ac.uk/">Computer Vision Group,  </a >
                     <a href="http://www.eecs.qmul.ac.uk/">School of Electronic Engineering and Computer Science,  </a >
                     <a href="http://www.qmul.ac.uk/">Queen Mary University of London</a >
               </div>
               <ul id="tabs">
                  <li><a href="https://qmul-survface.github.io/QMUL-SurvFace.html" name="#tab1">Home</a></li>
                  <li><a href="https://qmul-survface.github.io/QMUL-SurvFace_challenges.html" name="#tab2">Challenges</a></li>
                  <li><a href="https://qmul-survface.github.io/QMUL-SurvFace_results.html" name="#tab3">Results</a></li>
               </ul>
            </div>
            <h3 id="Benchmark">Data Partition</h3>
            <p>
               The same data partition is used for both (I) Face Identification and (II) Face Verification
               challenges. Specifically,
               the 10,638 persons with 2 or more face images are divided randomly into
               two halves: one half (5,319) as the training data, the
               other half (5,319). The remaining 4,935 single-shot
               identities (totally 10,254) are used for model test. 
               These statistics
               are summarsied in the Table below.
               The same data partition is applied to both
               face identification and face verification challenges.
            </p>
            <table align="center" width="70%" style="margin: 0px auto;" class="justify" cellpadding="0" cellspacing="0">
               <thead align="center">
                  <tr>
                     <th>Split</th>
                     <th>All</th>
                     <th>Training Set</th>
                     <th>Testing Set</th>
                  </tr>
               </thead>
               <tbody align="center">
                  <tr>
                     <td><strong>IDs</strong></td>
                     <td>15,573</td>
                     <td>5,319</td>
                     <td>10,254</td>
                  </tr>
                  <tr>
                     <td><strong>Images</strong></td>
                     <td>463,507</td>
                     <td>220,890</td>
                     <td>242,617</td>
                  </tr>
               </tbody>
            </table>
            <br>
            <h3 id="identification">(I) Face Identification</h3>
            <p>In this challenge, an <strong>open-set</strong> face recognition protocol is adopted.
               This is because in realistic surveillance applications, most faces captured by CCTV cameras 
               are not of any target persons and therefore should be detected as unknown.
               Specifically, a watch
               list identification scenario is created by building
               the probe and gallery sets as below: (1) Out of the 5,319
               multi-shot test identities, 3,000 are randomly selected for each of which
               half face images go into
               the gallery set, i.e. the watch list. (2) All the remaining
               images and the single-shot imagery are used to
               form the probe set.
               In the open-set evaluation setting,
               two error types are commonly used.
               The first type is <em>false alarm</em> quantified by the False Positive Identification
               Rate (FPIR):
            </p>
            <p><img style="width: 130px;" src="./images/fpir.png"></p>
            <p>which measures the proportion of nonmate searches <img style="width: 25px;" src="./images/Nnmm.png"/>
               (i.e. no mate faces in the gallery) that produce
               one or more enrolled candidates at or above a threshold
               t (i.e. false alarm), among a total of <img style="width: 25px;" src="./images/Nnm.png"/> nonmate
               searches attempted.
            </p>
            <p>The second type of error is <em>missing</em> quantified
               by the False Negative Identification Rate (FNIR):
            </p>
            <p><img style="width: 130px;" src="./images/fnir.png"></p>
            <p>which is the proportion of mate searches <img style="width: 25px;" src="./images/Nmnm.png"/> 
               (i.e. with mate faces present in the gallery) with enrolled mate
               found outside top r ranks or matching similarity score
               below the threshold t, among <img style="width: 25px;" src="./images/Nm.png"/> 
               mate searches.
            </p>
            A more intuitive measure may be the
            <em>hit rate</em> or True Positive Identification Rate (TPIR):
            <p><img style="width: 200px;" src="./images/tpir.png"></p>
            <p>Therefore, 
               the TPIR@FPIR measure is finally adopted as the open-set face identification performance metrics. 
            </p>
            <p>By varying the threashold, TPIR@FPIR can be used to generate an ROC curve.</p>
            <br>
            <h3 id="verification">(II) Face Verification</h3>
            <p>This challenge verifies if a pair of face images describe the same identity or not.
               Similar to other existing benchmarks (Huang et al, 2007, Klare et al, 2015), the sets of <em>matched</em> 
               and <em>unmatched</em> pairs are defined for performance evaluation.
               Specifically, for each of 5,319 test identities, a matched pair and an unmatched pair
               are randomly generated. For performance measurement on these pairs of testing algorithms, two types of error can occur
               This first one is <em>false accept</em> – a distractor claims an identity
               of interest, which is quatified by the False Accept Rate (FAR) to measure the fraction of unmatched pairs with the corresponding score s above threshold t:
            </p>
            <p><img style="width: 250px;" src="./images/far.png"/></p>
            <p>
               where <img style="width: 15px;" src="./images/u.png"/> denotes the set of unmatched pairs. 
            </p>
            <p> 
               This second one is <em>false reject</em> – the system mistakenly
               declines the identity of interest, which is quantified by 
               the False Rejection Rate (FRR) to measure the
               fraction of matched pairs with matching score s below a threshold t:
            </p>
            <p><img style="width: 250px;" src="./images/frr.png"/></p>
            <p>where <img style="width: 17px;" src="./images/m.png"/> is the set of matched pairs. Similar to TPIR,
               the True Accept Rate (TAR) is further defined to facilitate understanding as:
            </p>
            <p><img style="width: 160px;" src="./images/tar.png"/></p>
            <p>
               The TAR@FAR measurements are adopted for face verification evaluation.
            </p>
         </div>
      </div>
   </body>
</html>
